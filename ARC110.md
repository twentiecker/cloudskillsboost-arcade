# Create a Streaming Data Lake on Cloud Storage: Challenge Lab

## Task 1. Create a Pub/Sub topic

#### Use the command line to create a Pub/Sub topic called <TOPIC_ID>.

```bash
export TOPIC_ID=<TOPIC_ID>
```

```bash
gcloud pubsub topics create $TOPIC_ID
```

---

## Task 2. Create a Cloud Scheduler job

#### 1. Use the command line to create an App Engine app for your project.

```bash
export REGION=<REGION>
```

```bash
gcloud app create --region=$REGION
```

#### 2. Use the command line to create a Cloud Scheduler job in this project to publish messages at one-minute intervals to the Pub/Sub topic in task 1. Message body: <MESSAGE>.

```bash
export MESSAGE=<MESSAGE>
```

```bash
gcloud scheduler jobs create pubsub cloudlab --schedule="* * * * *" \
    --topic=$TOPIC_ID --message-body="$MESSAGE"
```

#### 3. Use the command line to start the scheduler job.

```bash
gcloud scheduler jobs run cloudlab
```

---

## Task 3. Create a Cloud Storage bucket

#### Use the command line to create a Cloud Storage bucket with the following bucket name: <BUCKET_NAME>

```bash
export BUCKET_NAME=<BUCKET_NAME>
```

```bash
gsutil mb gs://$BUCKET_NAME
```

---

## Task 4. Run a Dataflow pipeline to stream data from a Pub/Sub topic to Cloud Storage

#### Re-enabled Dataflow API

```bash
gcloud services disable dataflow.googleapis.com
```

```bash
gcloud services enable dataflow.googleapis.com
```

#### 1. Use the command line to create and run a Dataflow job to stream data from a Pub/Sub topic to a Cloud Storage bucket.

> Use Java or Python script as your choice. Sample code available on GitHub pages: java-docs-samples, python-docs-samples.

```bash
git clone https://github.com/GoogleCloudPlatform/java-docs-samples.git
cd java-docs-samples/pubsub/streaming-analytics
```

> Use the Pub/Sub topic that you created in a task 1.
> Use the Cloud Storage bucket that you created in task 3 as the output location.
> Group messages based on a fixed time window of 2 minutes.

```bash
export PROJECT_ID=<PROJECT_ID>
```

```bash
mvn compile exec:java \
-Dexec.mainClass=com.examples.pubsub.streaming.PubSubToGcs \
-Dexec.cleanupDaemonThreads=false \
-Dexec.args=" \
  --project=$PROJECT_ID \
  --region=$REGION \
  --inputTopic=projects/$PROJECT_ID/topics/$TOPIC_ID \
  --output=gs://$BUCKET_NAME/samples/output \
  --runner=DataflowRunner \
  --windowSize=2 \
  --tempLocation=gs://$BUCKET_NAME/temp"
```

#### 2. Use the command line to check which files have been written out in Cloud Storage.
