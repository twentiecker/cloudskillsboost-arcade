# Optimize Costs for Google Kubernetes Engine: Challenge Lab

## Task 1. Create a cluster and deploy your app

#### 1. Before you can deploy the application, you'll need to create a cluster in the <ZONE> zone, and name it as <CLUSTER_NAME>.

```bash
export ZONE=<ZONE>
```

```bash
export CLUSTER_NAME=<CLUSTER_NAME>
```

#### 2. Start small and make a zonal cluster with only two (2) nodes.

```bash
gcloud container clusters create $CLUSTER_NAME \
  --project=$DEVSHELL_PROJECT_ID \
  --zone=$ZONE \
  --machine-type=e2-standard-2 \
  --num-nodes=2
```

#### 3. Before you deploy the shop, make sure to set up some namespaces to separate resources on your cluster in accordance with the 2 environments - dev and prod.

```bash
kubectl create namespace dev
```

```bash
kubectl create namespace prod
```

#### 4. After that, deploy the application to the dev namespace with the provided command.

---

## Task 2. Migrate to an optimized node pool

#### 1. After successfully deploying the app to the dev namespace, take a look at the node details:

#### You come to the conclusion that you should make changes to the cluster's node pool:

#### There's plenty of left over RAM from the current deployments so you should be able to use a node pool with machines that offer less RAM.

#### Most of the deployments that you might consider increasing the replica count of will require only 100mcpu per additional pod. You could potentially use a node pool with less total CPU if you configure it to use smaller machines. However, you also need to consider how many deployments will need to scale, and how much they need to scale by.

#### 2. Create a new node pool named <POOL_NAME> with custom-2-3584 as the machine type.

```bash
export POOL_NAME=<POOL_NAME>
```

#### 3. Set the number of nodes to 2.

```bash
gcloud container node-pools create $POOL_NAME \
  --cluster=$CLUSTER_NAME \
  --machine-type=custom-2-3584 \
  --num-nodes=2 \
  --zone=$ZONE
```

#### 4. Once the new node pool is set up, migrate your application's deployments to the new nodepool by cordoning off and draining default-pool.

```bash
for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do  kubectl cordon "$node"; done
```

```bash
for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do kubectl drain --force --ignore-daemonsets --grace-period=10 "$node"; done
```

```bash
kubectl get pods -o=wide --namespace=dev
```

#### 5. Delete the default-pool once the deployments have safely migrated.

```bash
gcloud container node-pools delete default-pool --cluster $CLUSTER_NAME --zone $ZONE --quiet
```

---

## Task 3. Apply a frontend update

#### You just got it all deployed, and now the dev team wants you to push a last-minute update before the upcoming release! That's ok. You know this can be done without the need to cause down time.

#### 1. Set a pod disruption budget for your frontend deployment.

#### 2. Name it onlineboutique-frontend-pdb.

#### 3. Set the min-availability of your deployment to 1.

```bash
kubectl create poddisruptionbudget onlineboutique-frontend-pdb --selector app=frontend --min-available 1 --namespace dev
```

#### Now, you can apply your team's update. They've changed the file used for the home page's banner and provided you an updated docker image:

#### 4. Edit your frontend deployment and change its image to the updated one.

#### 5. While editing your deployment, change the ImagePullPolicy to Always.

```bash
kubectl patch deployment frontend -n dev --type=json -p '[
  {
    "op": "replace",
    "path": "/spec/template/spec/containers/0/image",
    "value": "gcr.io/qwiklabs-resources/onlineboutique-frontend:v2.1"
  },
  {
    "op": "replace",
    "path": "/spec/template/spec/containers/0/imagePullPolicy",
    "value": "Always"
  }
]'
```

---

## Task 4. Autoscale from estimated traffic

#### A marketing campaign is coming up that will cause a traffic surge on the OnlineBoutique shop. Normally, you would spin up extra resources in advance to handle the estimated traffic spike. However, if the traffic spike is larger than anticipated, you may get woken up in the middle of the night to spin up more resources to handle the load.

#### You also want to avoid running extra resources for any longer than necessary. To both lower costs and save yourself a potential headache, you can configure the Kubernetes deployments to scale automatically when the load begins to spike.

#### 1. Apply horizontal pod autoscaling to your frontend deployment in order to handle the traffic surge.

#### 2. Scale based on a target cpu percentage of 50.

#### 3. Set the pod scaling between 1 minimum and 10 maximum.

```bash
export MAX_REPLICAS=<MAX_REPLICAS>
```

```bash
kubectl autoscale deployment frontend --cpu-percent=50 --min=1 --max=$MAX_REPLICAS --namespace dev
```

#### Of course, you want to make sure that users won’t experience downtime while the deployment is scaling.

#### 4. To make sure the scaling action occurs without downtime, set the deployment to scale with a target cpu percentage of 50%. This should allow plenty of space to handle the load as the autoscaling occurs.

#### 5. Set the deployment to scale between 1 minimum and 10 maximum pods.

```bash
kubectl get hpa --namespace dev
```

#### But what if the spike exceeds the compute resources you currently have provisioned? You may need to add additional compute nodes.

#### 6. Next, ensure that your cluster is able to automatically spin up additional compute nodes if necessary. However, handling scaling up isn’t the only case you can handle with autoscaling.

#### 7. Thinking ahead, you configure both a minimum number of nodes, and a maximum number of nodes. This way, the cluster can add nodes when traffic is high, and reduce the number of nodes when traffic is low.

#### 8. Update your cluster autoscaler to scale between 1 node minimum and 6 nodes maximum.

```bash
gcloud beta container clusters update $CLUSTER_NAME --enable-autoscaling --min-nodes 1 --max-nodes 6 --zone=$ZONE
```
